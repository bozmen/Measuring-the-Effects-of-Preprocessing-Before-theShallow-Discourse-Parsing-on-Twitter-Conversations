{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from wordsegment import load, segment\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "import string \n",
    "\n",
    "load()\n",
    "\n",
    "def wsp_tokenizer(text):\n",
    "    return text.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2af3cb410e7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_opcodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m             print (\"%7s a[%d:%d] (%s) b[%d:%d] (%s)\" %\n\u001b[1;32m      3\u001b[0m                 (tag, i1, i2, 'a', j1, j2, 'a'))\n\u001b[1;32m      4\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'delete'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'equal'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessed_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sm' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "            for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
    "                        print (\"%7s a[%d:%d] (%s) b[%d:%d] (%s)\" %\n",
    "                            (tag, i1, i2, 'a', j1, j2, 'a'))\n",
    "                        if tag == 'delete' or tag == 'equal' or tag == 'replace':\n",
    "                            print('\\n', text[i1:i2], '\\n-----------------\\n', preprocessed_text[j1:j2], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, preprocessing_options):\n",
    "        self.po = preprocessing_options\n",
    "        if preprocessing_options.get('hashtag') or preprocessing_options.get('char_normalization'):\n",
    "            annotate = set()\n",
    "\n",
    "            if preprocessing_options.get('hashtag'):\n",
    "                if preprocessing_options.get('hashtag').get('replace'):\n",
    "                    annotate.add('hashtag')\n",
    "            if preprocessing_options.get('char_normalization'):\n",
    "                annotate.add('elongated')\n",
    "\n",
    "            annotate.add('censored')\n",
    "\n",
    "            self.text_preprocessor = TextPreProcessor(annotate=annotate,\n",
    "                # corpus from which the word statistics are going to be used for word segmentation \n",
    "                segmenter=\"twitter\", \n",
    "\n",
    "                # corpus from which the word statistics are going to be used for spell correction\n",
    "                corrector=\"twitter\", \n",
    "                unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "                unpack_contractions=False,  # Unpack contractions (can't -> can not)\n",
    "                spell_correct_elong=False,  # spell correction for elongated words\n",
    "\n",
    "                # select a tokenizer. You can use SocialTokenizer, or pass your own tokenizer, should take as input a string and return a list of tokens\n",
    "                tokenizer=wsp_tokenizer\n",
    "            )\n",
    "            \n",
    "    def preprocess_and_sync(self, input_file_path, output_file_path, input_ann_file_path, output_ann_file_path):\n",
    "        failures = []\n",
    "\n",
    "        with open(input_file_path) as input_file:\n",
    "            text = input_file.read()\n",
    "            preprocessed_text = self.preprocess(text)\n",
    "\n",
    "            # autojunk is too optimal\n",
    "            sm = SequenceMatcher(None, text, preprocessed_text, autojunk=False)\n",
    "            \n",
    "            for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
    "                print (\"%7s a[%d:%d] (%s) b[%d:%d] (%s)\" %\n",
    "                    (tag, i1, i2, 'a', j1, j2, 'a'))\n",
    "                if tag == 'delete' or tag == 'equal' or tag == 'replace':\n",
    "                    print('\\n', text[i1:i2], '\\n-----------------\\n', preprocessed_text[j1:j2], '\\n')\n",
    "\n",
    "            with open(output_file_path, 'w') as output_file:\n",
    "                output_file.write(preprocessed_text)\n",
    "            with open(input_ann_file_path, 'r', encoding='utf8') as input_ann_file:\n",
    "                input_ann_file_content = input_ann_file.read()\n",
    "                new_content = []\n",
    "                _start = 0\n",
    "                _end = 1\n",
    "                _in_number = False\n",
    "                ops = sm.get_opcodes()\n",
    "                for i, ann_char  in enumerate(input_ann_file_content):\n",
    "                    if not _in_number:\n",
    "                        if ann_char.isdigit():\n",
    "                            _start = i\n",
    "                            _in_number = True\n",
    "                        else:\n",
    "                            new_content.append(ann_char)\n",
    "                    else:\n",
    "                        if not ann_char.isdigit():\n",
    "                            _end = i\n",
    "                            _in_number = False\n",
    "                            if not input_ann_file_content[_start - 1].isalpha():\n",
    "                                num = int(input_ann_file_content[_start:_end])\n",
    "                                old_num = num\n",
    "                                actual_num = len(text.encode('utf-16le')[:(num * 2)].decode('utf-16le'))\n",
    "                                # print(num)\n",
    "                                new_num = 'NONE'\n",
    "                                for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
    "                                    if actual_num >= i1 and actual_num <= i2:\n",
    "                                        if tag == 'equal':\n",
    "                                            new_num = num - (i1 - j1)\n",
    "                                            break\n",
    "                                    if actual_num >= i1 and actual_num < i2:\n",
    "                                        if tag == 'delete':\n",
    "                                            new_num = j1\n",
    "                                            print('on delete:', actual_num, num, new_num)\n",
    "                                            break\n",
    "                                if new_num == 'NONE':\n",
    "                                    failures.append(input_file_path)\n",
    "                                new_content.append(str(new_num))\n",
    "                                new_content.append(ann_char)\n",
    "                            _start = _end + 1\n",
    "                            continue\n",
    "\n",
    "                with open(output_ann_file_path, 'w', encoding='utf8') as cleaned_ann:\n",
    "                    cleaned_ann.write(''.join(new_content))\n",
    "        if len(failures) > 0:\n",
    "            print(input_file_path, 'number of failures:', len(failures))\n",
    "                \n",
    "    def preprocess(self, text):\n",
    "        preprocessed_text = text\n",
    "        if self.po.get('remove_links'):\n",
    "            preprocessed_text = self.remove_links(preprocessed_text)\n",
    "        if self.po.get('remove_mentions'):\n",
    "            preprocessed_text = self.remove_mentions(preprocessed_text)\n",
    "\n",
    "        if self.po.get('hashtag') or self.po.get('char_normalization'):\n",
    "            preprocessed_text = self.preprocess_with_processor(preprocessed_text)\n",
    "\n",
    "        preprocessed_text = self.put_period(preprocessed_text)\n",
    "        return preprocessed_text\n",
    "\n",
    "    def remove_mentions(self, input_file, keep_inner=True):\n",
    "        new_lines = []\n",
    "        for line in input_file.split('\\n'):\n",
    "            words = line.split()\n",
    "            new_line = ''\n",
    "            for i, word in enumerate(words):\n",
    "                if not self.is_handle(word):\n",
    "                    if keep_inner:\n",
    "                        new_line = \" \".join(words[i:])\n",
    "                        break\n",
    "\n",
    "            new_lines.append(new_line + '\\n')\n",
    "        return ''.join(new_lines)\n",
    "\n",
    "    def remove_links(self, input_file):\n",
    "        new_lines = []\n",
    "        for line in input_file.split('\\n'):\n",
    "            words = line.split()\n",
    "            new_line = []\n",
    "            for word in words:\n",
    "                if not self.is_url(word):\n",
    "                    new_line.append(word)\n",
    "            if len(new_line) > 0:\n",
    "                new_lines.append(' '.join(new_line) + '\\n')\n",
    "        return ''.join(new_lines)\n",
    "\n",
    "    def replace_hashtags(self, input_file):\n",
    "        new_lines = []\n",
    "        for line in input_file.split('\\n'):\n",
    "            words = line.split()\n",
    "            new_line = []\n",
    "            for word in words:\n",
    "                ############\n",
    "                if word.startswith('#'):\n",
    "                    hashed_word = hash_fix(word)\n",
    "                    new_line.append(hashed_word)\n",
    "                else:\n",
    "                    new_line.append(word)\n",
    "                ############\n",
    "            if len(new_line) > 0:\n",
    "                new_lines.append(' '.join(new_line) + '\\n')\n",
    "        return ''.join(new_lines)\n",
    "    \n",
    "    def put_period(self, input_file):\n",
    "        new_lines = []\n",
    "        for line in input_file.split('\\n'):\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            elif not line[-1] in '?!.':\n",
    "                new_lines.append(line + '.')\n",
    "            else:\n",
    "                new_lines.append(line)\n",
    "        return '\\n'.join(new_lines)\n",
    "        \n",
    "\n",
    "    def preprocess_with_processor(self, text):\n",
    "        lines = text.split('\\n')\n",
    "        new_lines = []\n",
    "        for line in lines:  \n",
    "            preprocessed_text = self.text_preprocessor.pre_process_doc(line)\n",
    "            remove_words = ['<hashtag>','</hashtag>','<repeated>', '<elongated>', '<allcaps>' ,'</allcaps>']\n",
    "            filtered_words = list(filter(lambda w: w not in remove_words, preprocessed_text))\n",
    "            new_line = ' '.join(filtered_words)\n",
    "            new_lines.append(new_line)\n",
    "        return '\\n'.join(new_lines)\n",
    "\n",
    "    def hash_fix(self, hashtag):\n",
    "        new_hash = re.sub(r'#', '', hashtag)\n",
    "        tokens = segment(str(new_hash))\n",
    "        hashed = ' '.join(map(str, tokens)) \n",
    "        return hashed\n",
    "\n",
    "    def is_handle(self, word):\n",
    "        return word[0] == '@'\n",
    "\n",
    "    def is_url(self, word):\n",
    "        return re.match(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)', word) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     0,
     67,
     107,
     121,
     133,
     150,
     156,
     162,
     165
    ]
   },
   "outputs": [],
   "source": [
    "def preprocess_and_sync(input_file_path, output_file_path, input_ann_file_path, output_ann_file_path):\n",
    "    p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.EMOJI)\n",
    "    \n",
    "    failures = []\n",
    "    \n",
    "    with open(input_file_path) as input_file:\n",
    "        text = input_file.read()\n",
    "        preprocessed_text = preprocess(text, {\n",
    "            'remove_links': True,\n",
    "            'remove_mentions': True,\n",
    "            'hashtag': {\n",
    "                'replacement': True\n",
    "            },\n",
    "            'char_normalization': True\n",
    "        })\n",
    "        \n",
    "        # autojunk is too optimal\n",
    "        sm = SequenceMatcher(None, text, preprocessed_text, autojunk=False)\n",
    "        \n",
    "        with open(output_file_path, 'w') as output_file:\n",
    "            output_file.write(preprocessed_text)\n",
    "        with open(input_ann_file_path, 'r', encoding='utf8') as input_ann_file:\n",
    "            input_ann_file_content = input_ann_file.read()\n",
    "            new_content = []\n",
    "            _start = 0\n",
    "            _end = 1\n",
    "            _in_number = False\n",
    "            ops = sm.get_opcodes()\n",
    "            for i, ann_char  in enumerate(input_ann_file_content):\n",
    "                if not _in_number:\n",
    "                    if ann_char.isdigit():\n",
    "                        _start = i\n",
    "                        _in_number = True\n",
    "                    else:\n",
    "                        new_content.append(ann_char)\n",
    "                else:\n",
    "                    if not ann_char.isdigit():\n",
    "                        _end = i\n",
    "                        _in_number = False\n",
    "                        if not input_ann_file_content[_start - 1].isalpha():\n",
    "                            num = int(input_ann_file_content[_start:_end])\n",
    "                            old_num = num\n",
    "                            actual_num = len(text.encode('utf-16le')[:(num * 2)].decode('utf-16le'))\n",
    "                            # print(num)\n",
    "                            new_num = 'NONE'\n",
    "                            for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
    "                                if actual_num >= i1 and actual_num <= i2:\n",
    "                                    if tag == 'equal':\n",
    "                                        new_num = num - (i1 - j1)\n",
    "                                        break\n",
    "                                if actual_num >= i1 and actual_num < i2:\n",
    "                                    if tag == 'delete':\n",
    "                                        new_num = j1\n",
    "                                        print('on delete:', num, new_num)\n",
    "                                        break\n",
    "                            if new_num == 'NONE':\n",
    "                                failures.append(input_file_path)\n",
    "                            new_content.append(str(new_num))\n",
    "                            new_content.append(ann_char)\n",
    "                        _start = _end + 1\n",
    "                        continue\n",
    "            \n",
    "            with open(output_ann_file_path, 'w', encoding='utf8') as cleaned_ann:\n",
    "                cleaned_ann.write(''.join(new_content))\n",
    "    if len(failures) > 0:\n",
    "        print(input_file_path, 'number of failures:', len(failures))\n",
    "                \n",
    "def preprocess(text, preprocessing_options):\n",
    "    preprocessed_text = text\n",
    "    if preprocessing_options.get('remove_links'):\n",
    "        preprocessed_text = remove_links(preprocessed_text)\n",
    "    if preprocessing_options.get('remove_mentions'):\n",
    "        preprocessed_text = remove_mentions(preprocessed_text)\n",
    "        \n",
    "    if preprocessing_options.get('hashtag') or preprocessing_options.get('char_normalization'):\n",
    "        annotate = set()\n",
    "\n",
    "        if preprocessing_options.get('hashtag'):\n",
    "            if preprocessing_options.get('hashtag').get('replace'):\n",
    "                annotate.add('hashtag')\n",
    "        if preprocessing_options.get('char_normalization'):\n",
    "            annotate.add('elongated')\n",
    "            annotate.add('repeated')\n",
    "    \n",
    "        annotate.add('censored')\n",
    "\n",
    "        text_preprocessor = TextPreProcessor(annotate=annotate,\n",
    "            # corpus from which the word statistics are going to be used for word segmentation \n",
    "            segmenter=\"twitter\", \n",
    "\n",
    "            # corpus from which the word statistics are going to be used for spell correction\n",
    "            corrector=\"twitter\", \n",
    "            unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "            unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "            spell_correct_elong=False,  # spell correction for elongated words\n",
    "\n",
    "            # select a tokenizer. You can use SocialTokenizer, or pass your own tokenizer, should take as input a string and return a list of tokens\n",
    "            tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "\n",
    "            # list of dictionaries, for replacing tokens extracted from the text,with other expressions. You can pass more than one dictionaries.\n",
    "            dicts=[emoticons]\n",
    "        )\n",
    "        \n",
    "\n",
    "        \n",
    "    return preprocessed_text\n",
    "\n",
    "def remove_mentions(input_file, keep_inner=True):\n",
    "    new_lines = []\n",
    "    for line in input_file.split('\\n'):\n",
    "        words = line.split()\n",
    "        new_line = ''\n",
    "        for i, word in enumerate(words):\n",
    "            if not is_handle(word):\n",
    "                if keep_inner:\n",
    "                    new_line = \" \".join(words[i:])\n",
    "                    break\n",
    "                \n",
    "        new_lines.append(new_line + '\\n')\n",
    "    return ''.join(new_lines)\n",
    "\n",
    "def remove_links(input_file):\n",
    "    new_lines = []\n",
    "    for line in input_file.split('\\n'):\n",
    "        words = line.split()\n",
    "        new_line = []\n",
    "        for word in words:\n",
    "            if not is_url(word):\n",
    "                new_line.append(word)\n",
    "        if len(new_line) > 0:\n",
    "            new_lines.append(' '.join(new_line) + '\\n')\n",
    "    return ''.join(new_lines)\n",
    "\n",
    "def replace_hashtags(input_file):\n",
    "    new_lines = []\n",
    "    for line in input_file.split('\\n'):\n",
    "        words = line.split()\n",
    "        new_line = []\n",
    "        for word in words:\n",
    "            ############\n",
    "            if word.startswith('#'):\n",
    "                hashed_word = hash_fix(word)\n",
    "                new_line.append(hashed_word)\n",
    "            else:\n",
    "                new_line.append(word)\n",
    "            ############\n",
    "        if len(new_line) > 0:\n",
    "            new_lines.append(' '.join(new_line) + '\\n')\n",
    "    return ''.join(new_lines)\n",
    "\n",
    "def preprocess_with_processor(text, text_processor):\n",
    "    preprocessed_text = text_preprocessor.pre_process_doc(text)\n",
    "    remove_words = ['<hashtag>','</hashtag>','<repeated>', '<elongated>', '<allcaps>' ,'</allcaps>']\n",
    "    filtered_words = list(filter(lambda w: w not in remove_words, preprocessed_text))\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def hash_fix(hashtag):\n",
    "    new_hash = re.sub(r'#', '', hashtag)\n",
    "    tokens = segment(str(new_hash))\n",
    "    hashed = ' '.join(map(str, tokens)) \n",
    "    return hashed\n",
    "\n",
    "def is_handle(word):\n",
    "    return word[0] == '@'\n",
    "\n",
    "def is_url(word):\n",
    "    return re.match(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)', word) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/burak/anaconda3/envs/im/lib/python3.9/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    }
   ],
   "source": [
    "custom_preprocessor = Preprocessor({\n",
    "    'remove_links': True,\n",
    "    'remove_mentions': True,\n",
    "    'hashtag': {\n",
    "        'replacement': True\n",
    "    },\n",
    "    'char_normalization': True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_FOLDER = 'data/tw_data/PDTB_Annotations_20200205__'\n",
    "NEW_FOLDER = 'preprocessed_with_points'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/184 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "028_948124816611139589.branch318.txt.username_text_tabseparated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/184 [00:01<02:07,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136_950356330128314368.branch21.txt.username_text_tabseparated\n",
      "030_948126677648932869.branch4.txt.username_text_tabseparated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-31f2cbf48b6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         custom_preprocessor.preprocess_and_sync(\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mBASE_FOLDER\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/cleaned/threads/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mBASE_FOLDER\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mNEW_FOLDER\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/threads/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-9f05e2c68c50>\u001b[0m in \u001b[0;36mpreprocess_and_sync\u001b[0;34m(self, input_file_path, output_file_path, input_ann_file_path, output_ann_file_path)\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0m_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0m_in_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                 \u001b[0mops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_opcodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mann_char\u001b[0m  \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ann_file_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_in_number\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/im/lib/python3.9/difflib.py\u001b[0m in \u001b[0;36mget_opcodes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopcodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mai\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_matching_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m             \u001b[0;31m# invariant:  we've pumped out correct diffs to change\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0;31m# a[:i] into b[:j], and the next matching block is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/im/lib/python3.9/difflib.py\u001b[0m in \u001b[0;36mget_matching_blocks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0malo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mahi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbhi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_longest_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mahi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbhi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m             \u001b[0;31m# a[alo:i] vs b[blo:j] unknown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0;31m# a[i:i+k] same as b[j:j+k]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/im/lib/python3.9/difflib.py\u001b[0m in \u001b[0;36mfind_longest_match\u001b[0;34m(self, alo, ahi, blo, bhi)\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mbhi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m                 \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewj2len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj2lenget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbestsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m                     \u001b[0mbesti\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbestj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbestsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "files = os.listdir('data/tw_data/PDTB_Annotations_20200205__/cleaned/ann')\n",
    "for file_name in tqdm(files):\n",
    "    print(file_name)\n",
    "    try:\n",
    "        custom_preprocessor.preprocess_and_sync(\n",
    "            BASE_FOLDER + '/cleaned/threads/' + file_name,\n",
    "            BASE_FOLDER + '/' + NEW_FOLDER + '/threads/' + file_name,\n",
    "            BASE_FOLDER + '/cleaned/ann/' + file_name,\n",
    "            BASE_FOLDER + '/' + NEW_FOLDER + '/ann/' + file_name\n",
    "        )\n",
    "    except Exception as err:\n",
    "        print(file_name, err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  equal a[0:57] (a) b[0:57] (a)\n",
      "\n",
      " Any F/A’s want to form a team for Birmingham @IGFesports  \n",
      "-----------------\n",
      " Any F/A’s want to form a team for Birmingham @IGFesports  \n",
      "\n",
      " delete a[57:59] (a) b[57:57] (a)\n",
      "\n",
      "    \n",
      "-----------------\n",
      "  \n",
      "\n",
      "  equal a[59:70] (a) b[57:68] (a)\n",
      "\n",
      " Dm me if so \n",
      "-----------------\n",
      " Dm me if so \n",
      "\n",
      " delete a[70:71] (a) b[68:68] (a)\n",
      "\n",
      "   \n",
      "-----------------\n",
      "  \n",
      "\n",
      "  equal a[71:92] (a) b[68:89] (a)\n",
      "\n",
      "  Retweets Appreciated \n",
      "-----------------\n",
      "  Retweets Appreciated \n",
      "\n",
      " insert a[92:92] (a) b[89:90] (a)\n",
      "  equal a[92:93] (a) b[90:91] (a)\n",
      "\n",
      " \n",
      " \n",
      "-----------------\n",
      " \n",
      " \n",
      "\n",
      " delete a[93:138] (a) b[91:91] (a)\n",
      "\n",
      " @Axis_Fy @IGFesports @ShoutsAlot\n",
      "@SCreatiive  \n",
      "-----------------\n",
      "  \n",
      "\n",
      "  equal a[138:169] (a) b[91:122] (a)\n",
      "\n",
      " I don’t team with ppl with egos \n",
      "-----------------\n",
      " I don’t team with ppl with egos \n",
      "\n",
      " insert a[169:169] (a) b[122:123] (a)\n",
      "  equal a[169:170] (a) b[123:124] (a)\n",
      "\n",
      " \n",
      " \n",
      "-----------------\n",
      " \n",
      " \n",
      "\n",
      " delete a[170:191] (a) b[124:124] (a)\n",
      "\n",
      " @Axis_Fy @SCreatiive  \n",
      "-----------------\n",
      "  \n",
      "\n",
      "  equal a[191:209] (a) b[124:142] (a)\n",
      "\n",
      " And his is massive \n",
      "-----------------\n",
      " And his is massive \n",
      "\n",
      "replace a[209:213] (a) b[142:143] (a)\n",
      "\n",
      " eee\n",
      " \n",
      "-----------------\n",
      " . \n",
      "\n",
      "data/tw_data/PDTB_Annotations_20200205__/cleaned/threads/163_950370379897491456.branch1.txt.username_text_tabseparated number of failures: 1\n"
     ]
    }
   ],
   "source": [
    "BASE_FOLDER = 'data/tw_data/PDTB_Annotations_20200205__'\n",
    "FILE_NAME = '163_950370379897491456.branch1.txt.username_text_tabseparated'\n",
    "custom_preprocessor.preprocess_and_sync(\n",
    "    BASE_FOLDER + '/cleaned/threads/' + FILE_NAME,\n",
    "    BASE_FOLDER + '/' + NEW_FOLDER + '/threads/' + FILE_NAME,\n",
    "    BASE_FOLDER + '/cleaned/ann/' + FILE_NAME,\n",
    "    BASE_FOLDER + '/' + NEW_FOLDER + '/ann/' + FILE_NAME\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  equal a[0:5] (burak) b[0:5] (burak)\n",
      " insert a[5:5] () b[5:11] ( fakat)\n",
      "  equal a[5:12] ( ozmen ) b[11:18] ( ozmen )\n",
      "replace a[12:14] (bu) b[18:19] (o)\n",
      "  equal a[14:21] (radaydi) b[19:26] (radaydi)\n"
     ]
    }
   ],
   "source": [
    "sm = SequenceMatcher(None, a, b)\n",
    "\n",
    "for tag, i1, i2, j1, j2 in s.get_opcodes():\n",
    "    print (\"%7s a[%d:%d] (%s) b[%d:%d] (%s)\" %\n",
    "        (tag, i1, i2, a[i1:i2], j1, j2, b[j1:j2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OPT',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'api',\n",
       " 'clean',\n",
       " 'defines',\n",
       " 'enum',\n",
       " 'parse',\n",
       " 'preprocess',\n",
       " 'set_options',\n",
       " 'tokenize',\n",
       " 'utils']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
